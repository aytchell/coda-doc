<!doctype linuxdoc system>

<article>

<!-- Title information -->

<title>The Coda File Server
<author>Peter J. Braam, <tt/braam@cs.cmu.edu/
<date>v0.9, 25 Dec 1998

<!-- Table of contents -->
<toc>

<sect> Callback management in the server <P>
This code is in vicecb.cc. 
Callback management handles a hash table for FileEntry's and for
CallBackEntry's.  A CallBackEntry is attached/removed to each
FileEntry as needed. A CallBackEntry contains a pointer to the host
HostTable (the host (vicedep/srv.h) so that the host to notify can be
found back. All structures are managed in VM.  For locality of
referenence, CallBacks and FileEntry are allocated in blocks. Routines
are present to delete the callbacks for a Fid, or to remove all
backbacks for a Venus which has failed.

<sect> Volumes <P>

<sect1> Volume Data Structures<p>

When a server is initialized during its first run, a structure at a
fixed RVM address is initialized, from which all data access is
initiated. The structure is of type <em/coda_recoverable_segment/
defined in <tt/coda_globals.h/.  For volume handling it holds the
following information:

<enum>

<item> an array <em/VolumeList/ of size <em/MAXVOLS/ of <em/VolHead/
structures.  

<item> a constant <em/MaxVolId/ holding the maximum allocated volume
id. This constant is overloaded and holds the constant
<em/ThisServerId/ in its top 8 bits.
</enum>

We will now explore how the <em/VolHead/ structures lead to other
volume structures.  There are many data structures linking these
things together:

<descrip>

<tag/VolHead:/ defined in <tt/camprivate.h/. An RVM structure
containing a structure <em/VolumeHeader/ and <em/VolumeData/. 

<tag/VolumeData:/ defined in <tt/camprivate.h/. An RVM structure,
points to <em/VolumeDiskData/ and points to <em/SmallVnodeLists/ (with
some contstants related to these, and to <em/BigVnodeLists/.

<tag/VolumeHeader:/ defined in <tt/volume.h/. Contains the
<em/volumeid, type/ and <em/parentid./

<tag/VolumeDiskData:/ defined in <em/volume.h/. An RVM structure
holding the persistent data associated with the volume.

</descrip>

These RVM structures are copied into VM when a volume is accessed - we
will describe this in detail.  In VM we have a hash table
<em/VolumeHashTable/ for <em/Volume/ structures, with hash key the
<em/VolumeId/.  This is used in conjunction with a doubly linked list
<em/volumeLRU/ of <em/volHeader/ structures, which was probably
created to avoid keeping all <em/volHeader/'s in VM. 

<descrip>
<tag/Volume:/ defined in <tt/volume.h/. This structure is the
principal access point to a volume.  These VM structures are held in a
hash table. It contains quite a lot of information, such as a pointer to
a <em/volHeader/ (which has the cached RVM data), Device, partition,
vol_index, vnodeIndex, locks and other run time data.

<tag/volHeader:/ defined in <tt/volume.h/.  A VM structure sitting on
a dlist, with a backpointer to a <em/Volume/. Contains a
<em/VolumeDiskData/ structure. This is the VM cached copy of the RVM
<em/VolumeDiskData/ structure.
</descrip>

Notice that in RVM volume's are identified principally by their index,
which is the index in the static <em/VolumeList/ array of <em/volHead/
structures. Otherwise volumes are mostly accessed by their volume id.
To map an index to a volumeid proceeds through the <em/VolumeHeader/
structures held in the <em/VolumeList/ inside the <em/volHead/
structures.   

The reverse mapping, to get an index from a <em/VolumeId/ is done
through an auxiliary hashtable <em/VolTable/ of type <em/vhashtab/,
defined in <tt/volhash.cc/. 

It is informative to know the sizes of all these structures:
<itemize>
<item> VolumeDiskData: 636
<item> VolHead: 88
<item> volHeader: 648
<item> VolumeHeader: 20
<item> Volume: 96
</itemize>


<sect1> Initializing the volume package <p>

The VInitVolumePackage sets up a lot of other structures related to
volumes and vnodes. 
<descrip>

<tag/InitLRU:/ calloc a sequence of (normally 50) volHeader's, then
call ReleaseVolumeHeader to put it at the head of the volumeLRU.

<tag/InitVolTable:/ sets up a volhash table, hashing volid's to get
the index in the VolumeList.

<tag/VolumeHashTable:/ Hash table used to store pointers to the
Volume structure 
<tag/VInitVnodes:/ setup of the vnode VM lru caches for small and
large vnodes. Store summary
information in the VnodeClassInfoArray for both small and large
vnodes. The way to reach allocated vnode arrays is through the
VnodeClassInfoArray. 
<tag/InitLogStorage:/ go through the VolumeList and assign
VM resolution log memory for every volume; store the pointer
in VolLog&lsqb;i&rsqb;. The number of rlentries assigned is
VolumeList&lsqb i&rsqb;.data.volumeInfo-&gt;maxlogentries.
<tag/CheckVLDB:/ See if there is a VLDB. 
<tag/DP_Init/Find server partitions.
<tag/S_VolSalvage:/ this goes directly through the directory
contents, without invoking the buffer cache. 
<tag/FSYNC_fsInit:/ sets up a thread to watch over volume relocation
issues. No idea if this still works.
<tag/Attach volumes/    Now iterate through all volumes and call
VAttachVolumeById, to instantiate the VM volume information and attach
the volumes.  When 
attached, call InitVolLog which initialize pointers in VM for
resolution. The pointers give olists, whose elements correspond to the
vnode rec_smolists.  Finally create a VNResLog for each of the olist. 
entries. 
Set the RVM log transients for this volume.
<tag/VListVolumes/ write out the /vice/vol/VolumeList file (VListVolumes).
<tag/Vinit/ This variable is now set to 1. 
</descrip>

Attaching volumes is the following process. First a <em/VGetVolume/ is
done.  If it is found in the hash table and is in use, it is detached
with <em/VDetachVolume/ (error here is ignored).  Next the
<em/VolumeHeader/ is extracted from RVM.  If the program is running as
a <em/volumeUtility/ then <em/FSYNC_askfs/ is used to request the
volume.  We continue to chech the partition and call <em/attach2/. 

Attach2 allocated a new <em/Volume/ structure, initializes the locks
in this structure and fills in the partition information. A
<em/VolHeader/ is found from the LRU list and the routine
<em/VolDiskInfoById/ reads in the RVM data - this can only go wrong if
the volume id cannot be matched to a valid index.  If the needs
salvage field is set, we return the NULL pointer (leaving all memory
allocated). Attach2 continues by checking for a variety of bad
conditions, such as the volume apparently having crashed.  If it is
<em/blessed/, <em/in service/ and the <em/salvage/ flag is not set,
then we are ready to go and put the volume in the hash table. If the
volume is writable the bitmaps for vnodes are filled in.

<sect1> The FSYNC interface <p>

Requests for volume operations, such as <em/VGetVolume/ can come from: 

<itemize>
<item> The fileserver
<item> A volume utility
<item> The salvager
</itemize>

The FSYNC package allows a volume utility to register itself. The call
<em/VConnectFS/ is made during <em/VInitVolutil/ and makes available
an array for the calling thread, named <em/OffLineVolumes/. This array
is cleaned up during <em/VDisconnectFS/ and contains a list of volumes
that have been requested to be offline.  The requests that can be made
are <em/FSYNC_ON/ to get a volume back online.  This calls
<em/VAttachVolume/ and clears the spot in the OffLineVolumes.

The other request is <em/FSYNC_OFF/ and <em/FSYNC_NEEDVOLUME/.  If the 
volume is not yet in the threads <em/OffLineVolumes/ then a spot is
found for the volume. 

<sect1> Interface to the <em/volumeLRU/ <p>

This doubly linked list of <em/volHeader/'s is one of the more
confusing areas in the code.  This is the interface between VM and RVM
data, and the invariants are not very clear.

<descrip>

<tag/AvailVolumeHeader(struct Volume *)/ See if there is a
<em/volHeader/ available for the volume passed.

<tag/GetVolumeHeader(struct Volume *)/ Assign a <em/volHeader/ to the
<em/Volume/ structure. This routine does <bf/not/ fill in the data. 
</descrip>

<sect1> Interface to the <em/VolumeHashTable/. <p>

The when a volume is needed, a call is made to <em/VGetVolume/.  This
routine is given a <em/volumeid/ to search for and finds the
<em/Volume/ structure in the <em/VolumeHashTable/.  If this is not
found it returns <em/VNOVOL/.

If there are users of this volume already, then we are guaranteed that
the <em/VolumeHeader/ structure for this volume is available
too. Perhaps the <em/header/ field in the volume structure still
exists, otherwise a <em/volHeader/ must be found in the LRU, which
possibly involves writing an old one back to RVM.


 If
the <em/header/ field in the volume structure is not null we are ok,
if it is null we would like to use the first available header in the
<em/volumeLRU/ list.  This is done by checking the <em/back/ pointer
in the <em/volHeader/.


<em/Attaching a volume/ is described in AttachVolumebyId and
attach2. It means allocating a Volume structure in VM, finding its
partition, finding its VolHeader on the LRU, locating the
VolumeDiskData.  If the volume needs salvaging, or appears to be inuse
already, or not blessed don't attach it. Now insert the Volume
structure into the hash table, and initialize some vnode information
in the volume structure.

<sect2> Creating a volume <p>

The start of this is found in vol-create.cc in the volutil
directory. 
<enum>
<item> We first initialize the volumeid with VAllocateVolumeId.
<item> VCreateVolume (vol/vutil.cc) now build a VolumeDiskData
structure <em> vol </em> in VM.  The recoverable resolution log
vol.log is generated with recov_vol_log which does a RVMLIB_REC_MALLOC
to allocate a certain number of pointers to recle's. 
</enum>

<sect> Vnodes <p>

<sect1> Vnode data structures <p>

<enum>
<item> VnodeDiskObject (cvnode.h): RVM structure holding the following
information:
 <itemize>
 <item> type, mode, owner, mtime
 <item> inodeNumber: for a large vnode a pointer to a directory inode
in RVM for a small vnode a real disk inode number of a file. 
 <item> version vector
 <item> vol_index
 <item> rec_smolink nextvn: next vnode with the same vnodeindex
 <item> rec_dlist *log: pointer to resolution log
 </itemize>
<item> Vnode (cvnode.h): VM structure holding
 <itemize>
 <item> hashnext, lruNext, lruPrev, hashIndex
 <item> vnodeNumber
 <item> VolumePtr
 <item> VnodeDiskObject *
 </itemize>
<item> VnodeClassInfoArray: global VM variable with summary
information.
</enum>




<sect> Server RVM layout <p>
The initial RVM layout of a coda server is simple:

<code>
    boolean_t	    already_initialized;
    struct VolHead VolumeList[MAXVOLS];
    struct VnodeDiskObject    *SmallVnodeFreeList[SMALLFREESIZE];
    struct VnodeDiskObject    *LargeVnodeFreeList[LARGEFREESIZE];
    short    SmallVnodeIndex;
    short    LargeVnodeIndex;
    VolumeId    MaxVolId;
    long    Reserved[MAXVOLS];
</code>

These fields are accessed through the CAMLIB_REC macro. The
SmallVnodeIndex is the index of the highest pointer in the array,
pointing to a free vnode.  

The RVM layout is initialized in coda_init (recovc.cc) which allocates
small and large vnodes in RVM for each of the entries in the free
vnode list arrays. The vnodes are zeroed out. 

<sect1> Vnodes for volumes <p>

The VolumeData structure has a pointer to the arrays: smallVnodeLists
and largeVnodeLists. 

<sect1> Resolution Logs for vnode lists. <p>

<sect>The initialization of the volume package <p>

<verb>
VInitVolumePackage
    InitLRU
    InitVolTable
    VInitVnodes
    VInitVnodes
    InitLogStorage
    VCheckVLDB: checks header of disk VLDB, reports size
    check partitions and call: VInitPartition: puts partition in olist
    S_VolSalvage:
    FSYNC_fsInit: what are volume relocations????
    attaches all volumes:
      GetVolPartition: check consistency of index and volid in RVM, return parti
tions info.
      VAttachVolumeById
          Lookup in hash table
          if found take out of hash table
          sanity checks followed by 
          attach2
              callocs a volume structure
              GetVolumeHeader: take lru slot;  the vp header appears to maybe re
side in hash table 
              VoldDiskInfoById: link the vp with the RVM information (looks up t
he header in hashtable)
                    if this fails the volume is freed up again. 
              Tests if volume needs to be salvaged: ec set to VSALVAGE
              more of sanity and destroy
              AddVolumeToHashTable
              Set up uniquefier
              null VM bitmaps, then fill them in.
          If vp == NULL and other things: FSYNC_askfs else
          VUpdateVolume
              WriteVolumeHeader
              if error VForceOffline
          if error: VPutVolume
          VAddToVolumeUpdateList: update time stamps, calls VUpdateVolume (again
??)
          VAppenVolume is called to add to VolumeList if system is up and not li
sted yet.
      InitVolLog
      V_VolLog
    VListVolumes: dump partitions and volumehash table to VolumeList


Finding volumes by name:

ViceGetVolumeInfo
        VRDB.find by name
        if not found try to find by id (VRDB.find nasty casts)
        if found: call
             vrent::GetVolumeInfo for the entry 
                     fills in VSGADD stuff
                     special case for canonicalize
                     fills in the replica volid's in INFO           
        else 
             VGetVolumeInfo
                     gets it out of the VLDB_fd
             if successful and ROVOL: calculate GetVSGAddress
             elseif RWVOL lookup the volumeinfo for a replicated volume
        return information


VgetVolume and VPutVolume are probably like iget iput.

</verb>

<itemize>
<item> Volume databases (text) 
<item> <tt>/vice/vol/VolumeList</tt> Holds partition and volume
information for volumes for which this server is custodian.  This is
file is generated by srv.   
<item> <tt> /vice/vol/groupid</tt> is maximum replicated volume id
allocated so far.  
<item> HOSTS (text) 
<itemize>
<item> Entries describe volume storage groups: 
 id  hostname1 hostname2 etc.  Stored in /vice/db/VSGDB
<item> <tt>/vice/db/hosts</tt> servers should be in this file.  Servers found in
 the VSGDB are checked to be in this file before proceeding to make a volume. 
<item> <tt> /vice/db/servers</tt> contains an id for each server. 
</itemize>
</itemize>

Createvol_rep works as follows:
Parameters are: volname vsgaddr partition-name [groupid]
<itemize>
<item> check we are the SCM
<item> Check that the volname doesn't appear in AllVolumes or VRList
<item> Get the list of servers from the vsgaddr using the VSGDB
<item> verify the servers against /vice/db/hosts
<item> get a new replicated id for this replicated volume using the
<tt> /vice/vol/groupid</tt> file
<item> use volutil to create the volume on each host, using the
partition, volumename and groupid acquired before. Note that each server now
assigns a volume id for this newly created volume.  It dumps
</itemize>

<sect> 
bldvldb.sh <P>
<itemize>
<item> get <tt> /vice/vol/VolumeList </tt> from each server in <tt>
/vice/db/hosts</tt>. Names is server.list.new. First try this using
ftp, then rfs, then krcp. 
<item> if the file was acquired move it to <tt>
/vice/vol/remote/server.list</tt>
<item> dump them all in <tt>/vice/vol/BigVolumeList</tt>
<item> volutil rebuilds the VLDB with <tt> volutil makevldb
/vice/vol/BigVolumeList</tt> 
</itemize> 

Note this rebuilds the VLDB on the machine where is script is run,
i.e. the scm. 



</article>
