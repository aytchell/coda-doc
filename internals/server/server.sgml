<!doctype linuxdoc system>

<article>

<!-- Title information -->

<title>The Coda File Server
<author>Peter J. Braam, <tt/braam@cs.cmu.edu/
<date>v0.9, 25 Dec 1998

<!-- Table of contents -->
<toc>

<sect> Callback management in the server <P>

This code is in <tt/vicecb.cc/. 

The file server keeps a table of hosts that are up, see the section on
connection handling.  Each time a call back is granted to a client, a
file entry is entered in the hash table.  When the file is modified,
all clients holding the callback are notified.

Callback management handles a hash table for FileEntry's and for
CallBackEntry's.  A CallBackEntry is attached/removed to each
FileEntry as needed. 

A CallBackEntry contains a pointer to the host HostTable (the host
(vicedep/srv.h) so that the host to notify can be found back. All
structures are managed in VM.  For locality of referenence, CallBacks
and FileEntry are allocated in blocks. Routines are present to delete
the callbacks for a Fid, or to remove all backbacks for a Venus which
has failed.

<sect> Volumes <P>

<sect1> Volume Data Structures<p>

When a server is initialized during its first run, a structure at a
fixed RVM address is initialized, from which all data access is
initiated. The structure is of type <em/coda_recoverable_segment/
defined in <tt/coda_globals.h/.  For volume handling it holds the
following information:

<enum>

<item> an array <em/VolumeList/ of size <em/MAXVOLS/ of <em/VolHead/
structures.  

<item> a constant <em/MaxVolId/ holding the maximum allocated volume
id. This constant is overloaded and holds the constant
<em/ThisServerId/ in its top 8 bits.
</enum>

We will now explore how the <em/VolHead/ structures lead to other
volume structures.  There are many data structures linking these
things together:

<descrip>

<tag/VolHead:/ defined in <tt/camprivate.h/. An RVM structure
containing a structure <em/VolumeHeader/ and <em/VolumeData/. 

<tag/VolumeData:/ defined in <tt/camprivate.h/. An RVM structure,
points to <em/VolumeDiskData/ and points to <em/SmallVnodeLists/ (with
some contstants related to these, and to <em/BigVnodeLists/.

<tag/VolumeHeader:/ defined in <tt/volume.h/. Contains the
<em/volumeid, type/ and <em/parentid./

<tag/VolumeDiskData:/ defined in <em/volume.h/. An RVM structure
holding the persistent data associated with the volume.

</descrip>

These RVM structures are copied into VM when a volume is accessed - we
will describe this in detail.  In VM we have a hash table
<em/VolumeHashTable/ for <em/Volume/ structures, with hash key the
<em/VolumeId/.  This is used in conjunction with a doubly linked list
<em/volumeLRU/ of <em/volHeader/ structures, which was probably
created to avoid keeping all <em/volHeader/'s in VM, since the latter
are large.

<descrip>
<tag/Volume:/ defined in <tt/volume.h/. This structure is the
principal access point to a volume.  These VM structures are held in a
hash table. It contains quite a lot of information, such as a pointer to
a <em/volHeader/ (which has the cached RVM data), Device, partition,
vol_index, vnodeIndex, locks and other run time data.

<tag/volHeader:/ defined in <tt/volume.h/.  A VM structure sitting on
a dlist, with a backpointer to a <em/Volume/. Contains a
<em/VolumeDiskData/ structure. This is the VM cached copy of the RVM
<em/VolumeDiskData/ structure.
</descrip>

Notice that in RVM volume's are identified principally by their index,
which is the index in the static <em/VolumeList/ array of <em/volHead/
structures. Otherwise volumes are mostly accessed by their volume id.
To map an index to a volumeid proceeds through the <em/VolumeHeader/
structures held in the <em/VolumeList/ inside the <em/volHead/
structures.   

The reverse mapping, to get an index from a <em/VolumeId/ is done
through an auxiliary hashtable <em/VolTable/ of type <em/vhashtab/,
defined in <tt/volhash.cc/. 

It is informative to know the sizes of all these structures:
<itemize>
<item> VolumeDiskData: 636
<item> VolHead: 88
<item> volHeader: 648
<item> VolumeHeader: 20
<item> Volume: 96
</itemize>


<sect1> Initializing the volume package <p>

The VInitVolumePackage sets up a lot of other structures related to
volumes and vnodes. 
<descrip>

<tag/InitLRU:/ calloc a sequence of (normally 50) volHeader's, then
call ReleaseVolumeHeader to put it at the head of the volumeLRU.

<tag/InitVolTable:/ sets up the <em/VolTable/, hashing volid's to get
the index in the rvm <em/VolumeList/.

<tag/VolumeHashTable:/ Hash table used to lookup volumes by id. It
stores pointers to the Volume structure

<tag/VInitVnodes:/ setup of the vnode VM lru caches for small and
large vnodes. Store smmary information in the VnodeClassInfoArray for
both small and large vnodes. The way to reach allocated vnode arrays
is through the VnodeClassInfoArray.

<tag/InitLogStorage:/ go through the VolumeList and assign VM
resolution log memory for every volume; store the pointer in
VolLog&lsqb;i&rsqb;. The number of rlentries assigned is stored in:
<verb>
VolumeList[i].data.volumeInfo->maxlogentries.
</verb>

It is not yet clear if RVM resolution is using this.

<tag/CheckVLDB:/ See if there is a VLDB. 

<tag/DP_Init/Find server partitions.

<tag/S_VolSalvage:/ This analyzes all inodes on a Coda server
partition and matches data against directory contents. After this has
completed, the volume <em/InUse/ bit and <em/NeedsSalvage/ bit are
cleared (stored in the disk data).

<tag/FSYNC_fsInit:/ This interface is discussed below. 

<tag/Attach volumes/ Now iterate through all volumes and call
<em/VAttachVolumeById/. The details a far from clear, but the intent
is to add the information for each volume to the VM hash tables.

<tag/VListVolumes/ write out the <tt>/vice/vol/VolumeList</tt> file
(VListVolumes).  

<tag/Vinit/ This variable is now set to 1.
</descrip>

<sect1> Attaching volumes <p>

This code is sheer madness.

Attaching volumes is the following process. First a <em/VGetVolume/ is
done.  If this return a volume, it has found it in the hash table.  If
that volume is in use, it is detached with <em/VDetachVolume/ (error
here is ignored).  Attaching is not supposed to happen with anything
in the hashtable already.

Next the <em/VolumeHeader/ is extracted from RVM.  If the program is
running as a <em/volumeUtility/ then <em/FSYNC_askfs/ is used to
request the volume.  We continue to chech the partition and call
<em/attach2/.

Attach2 allocated a new <em/Volume/ structure, initializes the locks
in this structure and fills in the partition information. A
<em/VolHeader/ is found from the LRU list and the routine
<em/VolDiskInfoById/ reads in the RVM data - this can only go wrong if
the volume id cannot be matched to a valid index.  If the needs
salvage field is set, we return the NULL pointer (leaving all memory
allocated). Attach2 continues by checking for a variety of bad
conditions, such as the volume apparently having crashed.  If it is
<em/blessed/, <em/in service/ and the <em/salvage/ flag is not set,
then we are ready to go and put the volume in the hash table. If the
volume is writable the bitmaps for vnodes are filled in.

<em/DetachVolume/ is also very complicated.  It starts by taking the
volume out of the hash table (forcefully).  This means that
<em/VGetVolume/ will no longer find it.

The <em/shuttingDown/ flag is set to one, and <em/VPutVolume/ is
called. This frees the volume and <em/LWP_signal/s people waiting on
the <em/VPutVolume/ condition. [It is not clear that any process is
waiting, since they normally only appear to wait when the
<em/goingOffline/ flag is set, not when the <em/shuttingDown/ flag is
set.]


<sect1> State of volumes <p>

There are numerous flags indicating the state of volumes:

<descrip>

<tag/inUse/ Cleared by <em/attach2/, but only if volume is
<em/blessed/, not <em/needsSalvaged/ and <em/inService/.

<tag/inService/ The latter flag has some use in the volume utility
package, not much else.


<tag/goingOffline/ Taking a volume <em/offline/ (as in <em/Voffline/
means writing the volume header to disk, the <em/inUse/ bit turned
off.  A copy is kept around in VM.

Set by <em/Voffline/ <tag/shuttingDown/ used in <em/VDetachVolume/.

<tag/goingOffline/ Set by <em/VOffline/. Clear by <em/VPutVolume/,
<em/VForceOffline/ and <em/attach2/. In this case <em/VPutVolume/ sets
<em/inUse/ to 0, and <em/VForceOffline/ does that and sets the
<em/needsSalvaged/ flag.

<tag/blessed/ Heavily manipulated by volume utilities while creating
volumes, backing them up etc.  Probably indicates that the volume is
not in an internally consistent state.

<tag/needsSalvage/ This is an error condition. Cleared by
<em/VolSalvage/, set by <em/VForceOffline/.

</descrip>

<sect1> The FSYNC interface <p>

Requests for volume operations, such as <em/VGetVolume/ can come from: 

<itemize>
<item> The fileserver
<item> A volume utility
<item> The salvager
</itemize>

The FSYNC package allows a volume utility to register itself. The call
<em/VConnectFS/ is made during <em/VInitVolutil/ and makes available
an array for the calling thread, named <em/OffLineVolumes/. This array
is cleaned up during <em/VDisconnectFS/ and contains a list of volumes
that have been requested to be offline.  

The requests that can be made are <em/FSYNC_ON/ to get a volume back
online.  This clears the spot in the OffLineVolumes, calls
<em/VAttachVolume/ and finally writes out changes by calling
<em/VPutVolume/.

The other request is <em/FSYNC_OFF/ and <em/FSYNC_NEEDVOLUME/.  If the
volume is not yet in the threads <em/OffLineVolumes/ then a spot is
found for the volume.  If this is not for cloning the volume is marked
as <em/BUSY/ in the <em/specialStatus/ field of the <em/Volume/
structure.  In <em/VOffline/ the volume header is written to disk,
with the inUse bit turned off.  A copy of the header is maintained in
memory, however (which is why this is VOffline, not VDetach).

The FSYNC package can also watch over relocation sites. This is not
functional anymore, and should probably be removed.

<sect1> Interface to the <em/volumeLRU/ <p>

This doubly linked list of <em/volHeader/'s is one of the more
confusing areas in the code.  This is the interface between VM and RVM
data, and the invariants are not very clear.

<descrip>

<tag/AvailVolumeHeader(struct Volume *)/ See if there is a
<em/volHeader/ available for the volume passed.

<tag/GetVolumeHeader(struct Volume *)/ Assign a <em/volHeader/ to the
<em/Volume/ structure. This routine does <bf/not/ fill in the data. 
</descrip>

<sect1> Interface to the <em/VolumeHashTable/. <p>

The when a volume is needed, a call is made to <em/VGetVolume/.  This
routine is given a <em/volumeid/ to search for and finds the
<em/Volume/ structure in the <em/VolumeHashTable/.  If this is not
found it returns <em/VNOVOL/.

If there are users of this volume already, then we are guaranteed that
the <em/VolumeHeader/ structure for this volume is available
too. Perhaps the <em/header/ field in the volume structure still
exists, otherwise a <em/volHeader/ must be found in the LRU, which
possibly involves writing an old one back to RVM.

If the <em/header/ field in the volume structure is not null we are
ok, if it is null we would like to use the first available header in
the <em/volumeLRU/ list.  This is done by checking the <em/back/
pointer in the <em/volHeader/.


<sect1> Creating a volume <p>

The start of this is found in vol-create.cc in the volutil
directory. 
<enum>
<item> We first initialize the volumeid with VAllocateVolumeId.
<item> VCreateVolume (vol/vutil.cc) now build a VolumeDiskData
structure <em> vol </em> in VM.  The recoverable resolution log
vol.log is generated with recov_vol_log which does a RVMLIB_REC_MALLOC
to allocate a certain number of pointers to recle's. 
</enum>

<sect> Vnodes <p>

<sect1> Vnode data structures <p>

<enum>
<item> VnodeDiskObject (cvnode.h): RVM structure holding the following
information:
 <itemize>
 <item> type, mode, owner, mtime
 <item> inodeNumber: for a large vnode a pointer to a directory inode
in RVM for a small vnode a real disk inode number of a file. 
 <item> version vector
 <item> vol_index
 <item> rec_smolink nextvn: next vnode with the same vnodeindex
 <item> rec_dlist *log: pointer to resolution log
 </itemize>
<item> Vnode (cvnode.h): VM structure holding
 <itemize>
 <item> hashnext, lruNext, lruPrev, hashIndex
 <item> vnodeNumber
 <item> VolumePtr
 <item> VnodeDiskObject *
 </itemize>
<item> VnodeClassInfoArray: global VM variable with summary
information.
</enum>


<sect> Server RVM layout <p>
The initial RVM layout of a coda server is simple:

<code>
    boolean_t	    already_initialized;
    struct VolHead VolumeList[MAXVOLS];
    struct VnodeDiskObject    *SmallVnodeFreeList[SMALLFREESIZE];
    struct VnodeDiskObject    *LargeVnodeFreeList[LARGEFREESIZE];
    short    SmallVnodeIndex;
    short    LargeVnodeIndex;
    VolumeId    MaxVolId;
    long    Reserved[MAXVOLS];
</code>

These fields are accessed through the CAMLIB_REC macro. The
SmallVnodeIndex is the index of the highest pointer in the array,
pointing to a free vnode.  

The RVM layout is initialized in coda_init (recovc.cc) which allocates
small and large vnodes in RVM for each of the entries in the free
vnode list arrays. The vnodes are zeroed out. 

<sect1> Vnodes for volumes <p>

The VolumeData structure has a pointer to the arrays: smallVnodeLists
and largeVnodeLists. 

<sect1> Resolution Logs for vnode lists. <p>

<sect>VolInfo interface <p>

This should turn into the volume location server.

<verb>
ViceGetVolumeInfo
        VRDB.find by name
        if not found try to find by id (VRDB.find nasty casts)
        if found: call
             vrent::GetVolumeInfo for the entry 
                     fills in VSGADD stuff
                     special case for canonicalize
                     fills in the replica volid's in INFO           
        else 
             VGetVolumeInfo
                     gets it out of the VLDB_fd
             if successful and ROVOL: calculate GetVSGAddress
             elseif RWVOL lookup the volumeinfo for a replicated volume
        return information

</verb>

<sect> Configuration databases <p>

<itemize>
<item> Volume databases (text) 
<item> <tt>/vice/vol/VolumeList</tt> Holds partition and volume
information for volumes for which this server is custodian.  This is
file is generated by <bf/codasrv/.   
<item> <tt> /vice/vol/groupid</tt> is maximum replicated volume id
allocated so far.  
<item> HOSTS (text) 
<itemize>
<item> Entries describe volume storage groups: 
 id  hostname1 hostname2 etc.  Stored in /vice/db/VSGDB
<item> <tt>/vice/db/hosts</tt> servers should be in this file.  Servers found in
 the VSGDB are checked to be in this file before proceeding to make a volume. 
<item> <tt> /vice/db/servers</tt> contains an id for each server. 
</itemize>
</itemize>

Createvol_rep works as follows:
Parameters are: volname vsgaddr partition-name [groupid]
<itemize>
<item> check we are the SCM
<item> Check that the volname doesn't appear in AllVolumes or VRList
<item> Get the list of servers from the vsgaddr using the VSGDB
<item> verify the servers against /vice/db/hosts
<item> get a new replicated id for this replicated volume using the
<tt> /vice/vol/groupid</tt> file
<item> use volutil to create the volume on each host, using the
partition, volumename and groupid acquired before. Note that each server now
assigns a volume id for this newly created volume.  It dumps
</itemize>

<sect> 
bldvldb.sh <P>
<itemize>
<item> get <tt> /vice/vol/VolumeList </tt> from each server in <tt>
/vice/db/hosts</tt>. Names is server.list.new. First try this using
ftp, then rfs, then krcp. 
<item> if the file was acquired move it to <tt>
/vice/vol/remote/server.list</tt>
<item> dump them all in <tt>/vice/vol/BigVolumeList</tt>
<item> volutil rebuilds the VLDB with <tt> volutil makevldb
/vice/vol/BigVolumeList</tt> 
</itemize> 

Note this rebuilds the VLDB on the machine where is script is run,
i.e. the scm. 


</article>
