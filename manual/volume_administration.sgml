<chapter id="SysAdmVol">
<title>Volume Administration</>

<sect1>
<title>Concepts</>

<para>Although users view the Coda file system as a hierarchy of directories
and files, system administrators view the Coda file system as a hierarchy of volumes.  Each volume contains a subtree of related directories and files which is a subtree of the entire file system.  Volumes, then, parallel traditional Unix file systems.  Like a Unix file system, a volume can be mounted.  Thus, the root of a volume can be named within another volume at a mount point.  The Coda file system hierarchy is built in this manner and is then mounted by each client using a conventional Unix mount point within the local file system.  Since all internal mount points are invisible, the user sees only a single mount point for the entire Coda file system.  </para>

<para>All system administration tasks are performed relative to a volume or a set of volumes.  So, adding new users requires creating new volumes for their files and directories;  quotas are enforced on volumes; and backups are performed on a per-volume basis.  The volume abstraction greatly simplifies the administration of large systems.  </para>

<para>The Coda file system provides four different types of volumes.  The simplest of these is the <firstterm>non-replicated volume</>.  Non-replicated volumes reside on a single server and are in the custody of the Coda file server they reside on.  The Coda servers work with the <application>venus</> processes on client workstations to provide a single, seamless view of the file system.  However, if a custodian crashes or is otherwise inaccessible, its non-replicated volumes are inaccessible as well.  </para>

<para>Thus, the main volume type that is used in typical Coda environments are
the <firstterm>read-write, replicated volumes</>.  Read-write, replicated
volumes are logical volumes which group together multiple read-write,
non-replicated volumes.  Coda provides protocols which allow read-write,
replicated volumes to reside on a number of servers and to be accessed even
when some servers are inaccessible. </para>

<table id="VolTypes">
<title>Coda volume types</>
<tgroup cols=4>
<thead>
<row>
<entry>Volume Type</entry>
<entry>Where Reads are performed</entry>
<entry>Where Writes are performed</entry>
<entry>Conflicts</entry>
</row>
</thead>
<tbody>
<row>
<entry>Non-replicated</entry>
<entry>Only Custodian</entry>
<entry>Only Custodian</entry>
<entry>No</entry>
</row>
<row>
<entry>Read-Write Replicated</entry>
<entry>Any VSG Member</entry>
<entry>Any VSG Member</entry>
<entry>Yes</entry>
</row>
<row>
<entry>Backup</entry>
<entry>Only custodian</entry>
<entry>Nowhere</entry>
<entry>No</entry>
</row>
</tbody>
</tgroup>
</table>

<sect1 id="CreateVol">
<title>Creating a Volume</title>

<para>Typically, volumes consist of a single users data objects or other logically connected groups of data objects.  Four factors should be used in dividing the file system tree into volumes.  </para>

<itemizedlist>
<listitem><para>The volume is the unit of quota enforcement and, potentially, accounting.  </para></listitem>
<listitem><para>
The rename command is prohibited across volume boundaries.  Thus,
manipulation of the tree structure at a granularity other than a whole
volume (e.g. moving the mount point) or less than a volume (e.g. moving
directories or files within a volume) is expensive.  Moving a subtree of a 
volume to another volume requires copying every byte of data in the
subtree.
</para></listitem>
<listitem><para>
The size of the volume should be small enough that moving volumes
between partitions is a viable approach to balancing server disk
utilization and server load.  Thus, volumes should be small relative to
the partition size.
</para></listitem>
<listitem><para>
Finally, the size of a volume must not exceed the capability of the
backup media.  </para></listitem>
</itemizedlist>

<para>A volume naming convention should also be used by those administrators who create volumes.  Volume names are restricted to <literal>32</> characters and should be chosen so that given a volume name, a system administrator (who knows the naming conventions) can determine its correct location in the file system hierarchy.  The convention used by the Coda project is to name volumes by their function and location.  Thus, a replicated volume named <literal>u.hbovik</> is mounted in <filename>/coda/usr/hbovik</> and contains hboviks data.  A project volume is prefixed by <literal>p.</> and a system volume is prefixed by <literal>s.</>  Similarly, volumes containing machine specific object files are prefixed by the machine type.  For instance, <literal>p.c.alpha.pmax.bin</> contains project coda binaries for our current alpha release and is mounted on <filename>/coda/project/coda/alpha/pmax_mach/bin</>.  </para>

<para>Use the command
<citerefentry><refentrytitle>createvol_rep</><manvolnum>8</></citerefentry> to
create read-write replicated volumes respectively.
This command is actually a script which ultimately invokes the
<citerefentry><refentrytitle>volutil</><manvolnum>8</></citerefentry> command
with the create_rep option at the appropriate server.  The volume will contain
an access list initialized to <literal>System:Administrators rlidwka</> and
<literal>System:AnyUser rl</>.  Creating the volume does not mount the volume
within the file system hierarchy. Mounting the volume as well as changing the
access list or the quota must be done using the
<citerefentry><refentrytitle>cfs</><manvolnum>1</></citerefentry> command from
a client.  A new volume may not be visible at client workstations for some
time (see <xref linkend="VLDB"> below).  </para>

<para>A few concrete examples should clarify the use of some of these
commands. On the SCM,  the <command>createvol_rep</> <option>u.hbovik</>
<option>E0000107</> <option>/vicepa</> will create a replicated volume named
<literal>user.hbovik</> on each server in the Volume Server Group (VSG)
identified by <literal>E0000107</>.  The file <filename>/vice/db/VSGDB</>
contains the mapping between VSGs and servers. The names of the replicas will
be <literal>user.hbovik.</><replaceable>n</>, where <replaceable>n</> is a
number between <literal>0</> and <literal>|VSG| - 1</literal>.  </para>
</sect1>

<sect1 id="MountVolume">
<title>Mounting a Volume</title>

<para>In order to use a volume which you have created and added to the appropriate databases, you must mount the volume.  Although Unix file systems must be mounted upon reboot in Unix, Coda volumes are mounted only once.  To mount a Coda volume, you must be using a Coda client and be authenticated as a user (use the <command>clog</>) who has write access to the directory in which the mount point will be created.  </para>

<para>Mount the volumes using the command <command>cfs</> <option>mkmount</> <replaceable>filename</> <replaceable>volname</>. Note that <application>cfs</> creates <replaceable>filename</> automatically.  For example, <command>cfs</> <option>mkmount</> <option>/coda/usr/hbovik</> <option>u.hbovik</> will create <filename>/coda/usr/hbovik</> and then mount the <literal>u.hbovik</> volume created in the example in <xref linkend="CreateVol">.  The volume is now visible to all users of the Coda file system.  More information about the <application>cfs</> command can be found in <xref linkend="GettingStarted"> and in man page.  </para>

<Warning><para>When mounting a volume, avoid creating multiple mount points for it.  Coda cannot check for this.</para></warning>  
</sect1>

<sect1 id="DeleteVol">
<title>Deleting a Volume</>

<para>When a volume is no longer needed, it may be purged by running the 
<application>purgevol</application> or <application>purgevol_rep</application> scripts on the SCM.
Before removing a volume, you should probably create a backup for offline
storage (see <xref linkend="Dumping">).  The volume's mount point should be
removed with the
<citerefentry><refentrytitle>cfs</><manvolnum>1</></citerefentry> command (see
the <option>rmmount</> option) before purging the volume (if possible). Note
that purging the volume will not purge related backup volumes. Backup and
should be purged with the <application>purgevol</application> script or the
<command>volutil</> <option>purge</> command.
</sect1>

<sect1 id="Dumping">
<title>Dumping and Restoring a Volume</title>

<sect2>
<title>Creating a dump of a replicated volume</title> 

<para>On the SCM, you need to clone the read-write copy of the volume.  You
can use the command <command>volutil</> <option>clone</>
<option><replaceable>VolumeId</></option>. This command will create a
read-only volume with the name <literal>binaries.readonly</> (assuming that
your volume is called <literal>binaries</>).  Next, you will need to dump this
cloned volume to a file with the command <command>volutil</> <option>dump</>
<option><replaceable>VolumeId</></option>
<option><replaceable>filename</></option>. </para>

<para>The next section explains how to restore a dumped volume. </para>
</sect2>

<sect2>
<title>Restoring volume dumps</title>

<para>For complete details on the backup/restore process, see <xref
linkend="Backup">. In short, one first needs to get the correct dumpfile,
possibly merging differential dumps to an older full dump to get the desired
state to be restored. Once this file is obtained, use the
<citerefentry><refentrytitle>volutil</><manvolnum>8</></citerefentry> restore
facility.  </para> 

<para>
<cmdsynopsis>
  <command>volutil</>
    <arg choice=plain>restore</>
    <arg choice=req><replaceable>filename</></arg>
    <arg choice=req><replaceable>partition</></arg>
    <arg choice=opt><replaceable>volname</>
      <arg choice=opt><replaceable>volid</></arg>
    </arg>
</cmdsynopsis>
The <replaceable>partition</> should be one of the
<filename>/vicep?</filename> partitions on the server. You may optionally
specify the volumeId and the volume name for the restored volume. This is
useful when creating read-only replicated volumes.  Note that currently dump
files are not independent of byte ordering -- so volumes cannot be dumped
across architectures that differ in this respect.  </para>

<para>After the volume has been restored the volume location databases have to
be updated before client can see the restored volume. This is explained in
<xref linkend="VLDB">.
</sect2>
</sect1>

<sect1 id="VLDB">
<title>Building the volume databases</>

<formalpara>
<title>VLDB</>
<para>The <firstterm>Volume Location Data Base</>, <acronym>VLDB</>, is used to provide volume addressing information to workstations.  Copies of the VLDB reside on the servers and are updated periodically.  The VLDB lists the latest known location or locations of all on-line volumes in the system. A human readable version of the VRDB is maintained on the SCM in the file <filename>/vice/vol/VRList</filename>.  </para>
</formalpara>

<para>The VLDB is maintained on the SCM. When you wish to update it, run the
<citerefentry><refentrytitle>bldvldb.sh</><manvolnum>8</></citerefentry> from
<filename>/vice/bin/</> on the SCM.  The script gathers a copy of the
<filename>/vice/vol/VolumeList</filename> file from all the servers, merges it
into a single list, and builds a new VLDB. The <application>updateclnt</> and
<application>updatesrv</> daemons will then propagate the new VLDB to all the
servers.
</para>

<note><para>The <application>createvol_rep</application> and <application>purgevol_rep</application> scripts automatically invoke <application>bldvldb.sh</application>.  </para></note>

<formalpara>
<title>Building the VRDB</>
<para>The <firstterm>Volume Replication Data Base</>, the <acronym>VRDB</>, is used to provide information
about replicated volumes to client workstations.  Copies of the VRDB reside on all servers and are updated periodically.  The VRDB maps each logical volume to its corresponding set of physical volumes.  A human readable version of the VRDB is maintained on the SCM in the file <filename>/vice/vol/VRList</filename>.  The <option>makevrdb</> option to the <citerefentry><refentrytitle>volutil</><manvolnum>8</></citerefentry> command will create a new VRDB which will automatically be distributed to the other servers.  </para>
</formalpara>

<formalpara>
<title>Building the VSGDB</>
<para>The <firstterm>Volume Storage Group Data Base</>, <acronym>VSGDB</>, is created during setup process and, currently, is maintained by hand.  Each valid volume storage group has an entry in this data base containing an identification number and the names of the servers in the group.  </para>
</sect1>

<sect1>
<title>Ensuring Volume Consistency after Server Crashes</>

<para>Coda servers ensure file system consistency after a crash by running <citerefentry><refentrytitle>fsck</><manvolnum>8</></citerefentry>, recovering RVM, and running the Coda salvager.  The <application>fsck</application> used here a CMU has been modified so that it does not require every inode to be referenced, as Coda accesses inode directly.  </para>

<warning><para>The vanilla <application>fsck</application> must not be used on a Coda file system partition as the Coda files will be thrown away.</para></warning>

<para>After the server machine is booted, the <application>codasrv</application> process starts and RVM recovers the servers committed state.  The Coda salvager then reconciles the results from <application>fsck</application> and the salvager.  </para>
</sect1>

<sect1>
<title>Getting Volume Information</>

<para>The <application>cfs</application> provides information on volumes. <application>cfs</application> can only be used on a machine which has a running <application>venus</> (such as a client workstation).  <application>cfs</application> is described in <xref linkend="GettingStarted"> as well as in the manual page.  </para>
</sect1>
</chapter>
